{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze resident submitted move in work order description, identify common issues and patterns, prioritize urgent problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import  matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from textblob import TextBlob\n",
    "# from rake_nltk import Rake\n",
    "# from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pro_ALT_UP = pd.read_excel(\"UTD_2024_Propertyware_FS.xlsx\", sheet_name=\"Updated_Pro\")\n",
    "Pro_ALT_UP = pd.DataFrame(Pro_ALT_UP)\n",
    "Pro_ALT_UP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pro_ALT_UP[\"Description\"] = Pro_ALT_UP[\"Description\"].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Prepare Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# des = Pro_ALT_UP['Description'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = Pro_ALT_UP['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = des.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = des.str.replace('[^\\w\\s]', '', regex=True) # remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = des.str.replace('\\d+', '', regex=True) # remove numbers like 1. 2. .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pro_ALT_UP[\"Description\"] = Pro_ALT_UP[\"Description\"].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_tokens = des.apply(lambda x: x.split()) # Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# des_tokens = des.apply(lambda x: [token.text for token in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_words = {'need', 'one', 'working','work', 'issue', 'problem','fix', 'require','also','yes','please','coming','open'}\n",
    "stop_words = ENGLISH_STOP_WORDS.union(irrelevant_words) # remove common stop words like \"the\", \"is\", \"and\"..\n",
    "des_tokens = des_tokens.apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() # get words back to root\n",
    "des_tokens = des_tokens.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Word Freq Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for tokens in des_tokens for word in tokens]\n",
    "word_freq = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = word_freq.most_common(20)\n",
    "for word, freq in top_words:\n",
    "    print(f\"{word}:{freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vis\n",
    "words, freqs = zip(*top_words)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(words, freqs)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 20 Most Freq Words in WO Des')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Phrase Analysis (N-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Bigrams\n",
    "def generate_ngrams(tokenized_description, n=2):\n",
    "    all_ngrams = []\n",
    "    for tokens in tokenized_description:\n",
    "        all_ngrams.extend(list(ngrams(tokens, n)))\n",
    "    return all_ngrams\n",
    "\n",
    "bigrams = generate_ngrams(des_tokens, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = generate_ngrams(des_tokens, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_freq = Counter(bigrams)\n",
    "trigrams_freq = Counter(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show most common ones (Bi)\n",
    "top_bigrams = bigrams_freq.most_common(10)\n",
    "print(\"Top 10 Bigrams:\")\n",
    "for bigram, freq in top_bigrams:\n",
    "    print(f\"{bigram}:{freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show most common ones (tri)\n",
    "top_trigrams = trigrams_freq.most_common(10)\n",
    "print(\"Top 10 Trigrams:\")\n",
    "for trigram, freq in top_trigrams:\n",
    "    print(f\"{trigram}:{freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vis\n",
    "bigram_words, bigram_counts = zip(*top_bigrams)\n",
    "bigram_words = [' '.join(bigram) for bigram in bigram_words]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(bigram_words, bigram_counts)\n",
    "plt.xlabel('Bigram')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Bigrams')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think things related with pet are not useful\n",
    "pet_keywords = {'pet','dog','cat','pets','animal','puppy','kitten','pte','just','moved'}\n",
    "def filter_ngrams(ngrams, pet_keywords):\n",
    "    return [(ngram, freq) for ngram, freq in ngrams if not any(word in pet_keywords for word in ngram)]\n",
    "\n",
    "filtered_bigrams = filter_ngrams(bigrams_freq.most_common(), pet_keywords)\n",
    "filter_trigrams = filter_ngrams(trigrams_freq.most_common(), pet_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show most common ones (Bi)\n",
    "# Without Pet\n",
    "top_f_bigrams = filtered_bigrams[:10]\n",
    "print(\"\\nTop 10 Filtered Bigrams:\")\n",
    "for bigram, freq in top_f_bigrams:\n",
    "    print(f\"{bigram}:{freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show most common ones (Bi)\n",
    "top_f_trigrams = filter_trigrams[:10]\n",
    "print(\"\\nTop 10 Filtered Trigrams:\")\n",
    "for trigram, freq in top_f_trigrams:\n",
    "    print(f\"{trigram}:{freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vis\n",
    "f_bigram_words, f_bigram_counts = zip(*top_f_bigrams)\n",
    "f_bigram_words = [' '.join(bigram) for bigram in f_bigram_words]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(f_bigram_words, f_bigram_counts)\n",
    "plt.xlabel('Filtered Bigram')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Filtered Bigrams')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vis\n",
    "f_trigram_words, f_trigram_counts = zip(*top_f_trigrams)\n",
    "f_trigram_words = [' '.join(trigram) for trigram in f_trigram_words]\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.bar(f_trigram_words, f_trigram_counts)\n",
    "plt.xlabel('Filtered Trigram')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Filtered Trigrams')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group words often shown together into one group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_des = des_tokens.apply(lambda x: ' '.join(x))\n",
    "# default_stop = ENGLISH_STOP_WORDS\n",
    "# custom_stop = {'need', 'one', 'working','work', 'issue', 'problem','fix', 'require','pte','pet'}\n",
    "# c_stop_words = default_stop.union(custom_stop)\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.85, min_df=2, stop_words = 'english')\n",
    "doc_term_matrix = vectorizer.fit_transform(cleaned_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pro_ALT_UP['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA\n",
    "num_topic = 3\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topic, random_state=42)\n",
    "lda_model.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Define function to display topics\n",
    "def display_topics(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx+1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))\n",
    "        \n",
    "display_topics(lda_model, feature_names, n_top_words=10) # display top 10 words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign topics to des\n",
    "topic_assign = lda_model.transform(doc_term_matrix)\n",
    "\n",
    "Pro_ALT_UP[\"Assigned Topic\"] = topic_assign.argmax(axis=1) +1\n",
    "print(Pro_ALT_UP[['Description','Assigned Topic']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_prop = lda_model.transform(doc_term_matrix).mean(axis=0)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(range(1, num_topic+1), topic_prop)\n",
    "plt.xlabel('Topic Number')\n",
    "plt.ylabel('Proportion of Des')\n",
    "plt.title('Topic Proportions Across All Des')\n",
    "plt.xticks(range(1, num_topic+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wordcloud for topic\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    topic_words = ' '.join([feature_names[i] for i in topic.argsort()[:-50-1:-1]])\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(topic_words)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Topic{topic_idx + 1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Clustering Similar Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(cleaned_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cluster = 5\n",
    "kmeans = KMeans(n_clusters=num_cluster, random_state=42)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pro_ALT_UP['Cluster'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See des for each cluster\n",
    "for cluster in range(num_cluster):\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    print(Pro_ALT_UP[Pro_ALT_UP['Cluster'] ==  cluster]['Description'].head(10).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top words in each cluster\n",
    "for cluster in range(num_cluster):\n",
    "    cluster_des = cleaned_des[Pro_ALT_UP['Cluster'] == cluster]\n",
    "    cluster_words = \" \".join(cluster_des).split()\n",
    "    print(f\"\\nCluster {cluster} Common Words:\")\n",
    "    print(Counter(cluster_words).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(tfidf_matrix.toarray())\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(reduced_data[:,0], reduced_data[:, 1], c=cluster_labels, alpha=0.7)\n",
    "plt.colorbar()\n",
    "plt.title('Scatter Plot of Cluster')\n",
    "plt.xlabel(\"PCA of Dim 1\")\n",
    "plt.ylabel(\"PCA of Dim 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_size = Pro_ALT_UP['Cluster'].value_counts()\n",
    "cluster_size.plot(kind='bar', figsize=(10,6))\n",
    "plt.title(\"Cluster Size\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Number of Des\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Emotional Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sen(description):\n",
    "    analysis = TextBlob(description)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "Pro_ALT_UP['Sentiment'] = cleaned_des.apply(get_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_issues = Pro_ALT_UP[Pro_ALT_UP['Sentiment'] < 0]\n",
    "print(\"Top Negative Issues:\")\n",
    "print(negative_issues[['Description', 'Sentiment']].sort_values(by='Sentiment').head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vis\n",
    "plt.figure(figsize=(10,6))\n",
    "Pro_ALT_UP['Sentiment'].hist(bins=20, color='skyblue')\n",
    "plt.title('Sentiment Score Dis')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Number of Des')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add flag based on negative sentiment\n",
    "Pro_ALT_UP['Priority'] = Pro_ALT_UP['Sentiment'].apply(lambda x: 'High' if x < -0.5 else('Medium' if x < 0 else 'Low'))\n",
    "high_priority = Pro_ALT_UP[Pro_ALT_UP['Priority'] == 'High']\n",
    "print(high_priority[['Description','Sentiment','Priority']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pro_ALT_UP['Month'] = pd.to_datetime(Pro_ALT_UP['Date Created']).dt.month # get months out\n",
    "monthly_sent = Pro_ALT_UP.groupby('Month')['Sentiment'].mean()\n",
    "monthly_sent.plot(kind='line', figsize=(10,6))\n",
    "plt.title('Average Sentiment Over Time')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Sentiment Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Key Words for Each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()\n",
    "def extract_kw_keybert(description):\n",
    "    keywords = kw_model.extract_keywords(des, top_n=5)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "Pro_ALT_UP['KeyBERT'] = cleaned_des.apply(lambda x: extract_kw_keybert(x))\n",
    "print(Pro_ALT_UP[['Description', 'KeyBERT']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
